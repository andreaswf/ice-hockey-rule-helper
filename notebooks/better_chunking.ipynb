{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d5791e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader, PDFMinerLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "from langsmith import traceable\n",
    "\n",
    "from ragas import EvaluationDataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import faiss\n",
    "\n",
    "# openAI embeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# vector store\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# load api keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b2e2652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load document\n",
    "file = \"../documents/2025-26_iihf_rulebook.pdf\"\n",
    "loader = PyPDFLoader(file)\n",
    "docs = loader.load()\n",
    "\n",
    "#drop TOC and appendix\n",
    "docs_cropped = docs[15:160]\n",
    "\n",
    "# sticth docs back together\n",
    "parts = []\n",
    "for i, d in enumerate(docs_cropped, start=1):\n",
    "    parts.append(f\"\\n\\n<<<PAGE {i}>>>\\n{d.page_content.strip()}\")\n",
    "\n",
    "merged_text = \"\".join(parts)\n",
    "\n",
    "# Create a Document and update metadata\n",
    "merged_doc = [Document(\n",
    "    page_content=merged_text,\n",
    "    metadata={\n",
    "        \"source\": \"IIHF Rulebook 2025-26\",\n",
    "        \"page_count\": len(docs_cropped),\n",
    "    }\n",
    ")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a58dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(merged_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "552648a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for spltting\n",
    "MAIN_RE = re.compile(r\"RULE[ \\u00A0]+(?P<main_id>\\d{1,3})[ \\u00A0]+(?P<main_name>[A-Z-'´’–/”“]{3,}+(?:[ \\u00A0][A-Z-'´’–/”“]+)*)\")\n",
    "SUB_RE = re.compile(r\"(?P<sub_id>\\d{1,3}\\.\\d{1,2})[.\\u00A0 ]*[ \\u00A0]+(?P<sub_name>[A-Z-'´’–/”“]{2,}+(?:[ \\u00A0][A-Z-'´’–/”“0-9]+)*)\")\n",
    "\n",
    "\n",
    "def normalize_ocr(text):\n",
    "    # collapse weird spaces (regular + non-breaking)\n",
    "    text = re.sub(r\"[ \\u00A0]+\", \" \", text)\n",
    "\n",
    "    # fix error for 'penalty' which somtimes is 'penal ty' etc\n",
    "    text = re.sub(r\"\\bPENAL\\s*TY\\b\", \"PENALTY\", text)\n",
    "    text = re.sub(r\"\\bPENAL\\s*TIES\\b\", \"PENALTIES\", text)\n",
    "    text = re.sub(r\"\\bAL\\s*TERNATE\\b\", \"ALTERNATE\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# add rule id and name to metadata\n",
    "def add_rule_metadata(m, prefix):\n",
    "    metadata = {\n",
    "        f\"{prefix}_rule\": m.group(0),\n",
    "        f\"{prefix}_rule_id\": m.group(1),\n",
    "        f\"{prefix}_rule_name\": m.group(2),\n",
    "    }\n",
    "    return metadata\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# split on rules and subrules\n",
    "def slice_on_regex(docs: list[Document], pattern, prefix):\n",
    "    out = []\n",
    "    rx = re.compile(pattern)\n",
    "\n",
    "    for d in docs:\n",
    "        text = normalize_ocr(d.page_content).lstrip()\n",
    "        matches = list(rx.finditer(text))\n",
    "        \n",
    "        # returns original document if no matches.\n",
    "        if not matches:\n",
    "            out.append(d)\n",
    "            continue\n",
    "        \n",
    "        # For each regex match, slice from its start up to the next match (or end of text),\n",
    "        # so each chunk corresponds to one rule section with its content. \n",
    "        for i, m in enumerate(matches):\n",
    "            if i + 1 < len(matches):\n",
    "                end = matches[i+1].start()  \n",
    "            else:\n",
    "                end = len(text)\n",
    "            chunk = text[m.start():end]\n",
    "            \n",
    "            # get new rule metadata and update metadata for the chunk\n",
    "            rule_metadata = add_rule_metadata(m, prefix)\n",
    "            new_meta = d.metadata.copy()\n",
    "            new_meta.update(rule_metadata)\n",
    "            \n",
    "            out.append(Document(page_content=chunk, metadata=new_meta))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb19ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_split = slice_on_regex(merged_doc, MAIN_RE, prefix='main')\n",
    "sub_split = slice_on_regex(main_split, SUB_RE, prefix='sub')\n",
    "\n",
    "\n",
    "#embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "embedding_dim = len(embeddings.embed_query(\"dim\"))\n",
    "\n",
    "# vectorstore\n",
    "vs_index = faiss.IndexFlatL2(embedding_dim)\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=vs_index,\n",
    "    docstore=InMemoryDocstore({}),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "# parent store\n",
    "parent_store = InMemoryStore()\n",
    "\n",
    "# child splitter (splits subrules into smalle chunks)\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap = 120\n",
    ")\n",
    "\n",
    "# saves the parent references to the smaller child chunks for retrivial.\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=parent_store,\n",
    "    child_splitter=child_splitter,\n",
    "    search_kwargs={\"k\": 8}\n",
    ")\n",
    "\n",
    "\n",
    "retriever.add_documents(sub_split)\n",
    "vectorstore.save_local(\"../vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01eeecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation step\n",
    "\n",
    "# formatting for llm context\n",
    "def format_docs(docs):\n",
    "    parts = []\n",
    "    for doc in docs:\n",
    "        meta = {\n",
    "            \"main_rule_id\": doc.metadata.get(\"main_rule_id\", \"N/A\"),\n",
    "            \"main_rule_name\": doc.metadata.get(\"main_rule_name\", \"N/A\"),\n",
    "            \"sub_rule_id\": doc.metadata.get(\"sub_rule_id\", \"N/A\"),\n",
    "            \"sub_rule_name\": doc.metadata.get(\"sub_rule_name\", \"N/A\"),\n",
    "            \"source\": doc.metadata.get(\"source\", \"N/A\")\n",
    "        }\n",
    "        parts.append(f\"Metadata: {meta}\\n{doc.page_content.strip()}\")\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "# defineing the system template\n",
    "system_template = \"\"\"You are an ice hockey rule assistant.\n",
    "\n",
    "Follow these rules:\n",
    "- Answer ONLY using the provided context below. If the answer is unknown or not in the context, say \"I don't know\".\n",
    "- Use bulletpoints. After each bullet, include a citation using the metadata main rule and sub rule\n",
    "\n",
    "\n",
    "Rule citation format:\n",
    "[<sub_rule_id> <main_rule_name> - <sub_rule_name>]\n",
    "\n",
    "\n",
    "Context (use only what is inside the markers):\n",
    "---\n",
    "{context}\n",
    "---\"\"\"\n",
    "\n",
    "# defining the prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"user\", \"Original question: {question}\"),\n",
    "])\n",
    "\n",
    "\n",
    "# retriever\n",
    "#retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "\n",
    "\n",
    "# llm\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "multi_query_system_template = \"\"\"You are an AI language model assistant. Your task is\n",
    "    to generate 3 different versions of the given user\n",
    "    question to retrieve relevant documents from a vector database.\n",
    "    By generating multiple perspectives on the user question,\n",
    "    your goal is to help the user overcome some of the limitations\n",
    "    of distance-based similarity search. Provide these alternative\n",
    "    questions separated by newlines. The questions are all about ice hockey.\n",
    "\"\"\"\n",
    "\n",
    "multi_query_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", multi_query_system_template),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "\n",
    "multi_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm, include_original=True, prompt=multi_query_prompt_template)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "@traceable\n",
    "def rag_bot(question: str):\n",
    "    # retrieve\n",
    "    docs = multi_retriever.invoke(question)\n",
    "    \n",
    "    # build context and format docs\n",
    "    context = format_docs(docs)\n",
    "    \n",
    "    # call rag_chain\n",
    "    answer = rag_chain.invoke({\"question\": question, \"context\": context})\n",
    "    \n",
    "    return {\"answer\": answer, \"documents\": docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a973870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c22663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag test\n",
    "#result = rag_bot(\"If the puck is shot before the red line, is touched after the red line and the goalie freezes it, can the defensive team change?\")\n",
    "result = rag_bot(\"Can i score a goal with my nose?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ebbb1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- A goal cannot be scored when the puck has been directed with any part of the body (excluding skates) into the net by an attacking Player other than with a stick. If it is deemed to be done deliberately, then the decision shall be NO GOAL. [78.5 GOALS - DISALLOWED GOALS]\n"
     ]
    }
   ],
   "source": [
    "print(result.get('answer'))\n",
    "#result.get('documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d19363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- No, the defensive team shall not be permitted to make any Player substitutions prior to the “face-off” in this situation. <63.1 DELAYING THE GAME> \n",
    "#- However, they can substitute a Player to replace an injured Player or when a penalty has been assessed which affects the “on-ice strength” of either team. <82.1 LINE CHANGE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e1b9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data for ragas testing\n",
    "\n",
    "# load file\n",
    "test_questions = []\n",
    "with open(\"eval_questions.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        test_questions.append(json.loads(line))\n",
    "             \n",
    "dataset = []\n",
    "for example in test_questions:\n",
    "    # extract from eval_questions\n",
    "    id = example.get('id')\n",
    "    question = example.get('question')\n",
    "    ground_truth = example.get('ground truth')\n",
    "    inner_rule = example.get('inner rule')\n",
    "    \n",
    "    # run the ragbot and get output answer\n",
    "    rag_bot_output = rag_bot(question)\n",
    "    answer = rag_bot_output.get('answer')\n",
    "    \n",
    "    # get context\n",
    "    docs = rag_bot_output.get('documents')\n",
    "    contexts = [doc.page_content for doc in docs]\n",
    "    \n",
    "    dataset.append({\n",
    "        \"user_input\": question,\n",
    "        \"retrieved_contexts\": contexts,\n",
    "        \"response\": answer,\n",
    "        \"reference\": ground_truth,\n",
    "    })\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee536d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860502f6184b4e2ab890178e5315ef15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.8571, 'faithfulness': 0.8413, 'factual_correctness(mode=f1)': 0.7129}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],\n",
    "    llm=llm)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4897c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'context_recall': 0.8571, 'faithfulness': 0.8571, 'factual_correctness(mode=f1)': 0.6186} # single query with top 4 docs retrieved\n",
    "{'context_recall': 0.8571, 'faithfulness': 0.8608, 'factual_correctness(mode=f1)': 0.6614} # single query with top 6 docs retrieved\n",
    "{'context_recall': 0.8571, 'faithfulness': 0.7579, 'factual_correctness(mode=f1)': 0.6886} # single query with top 8 docs retrieved\n",
    "{'context_recall': 0.8571, 'faithfulness': 0.7540, 'factual_correctness(mode=f1)': 0.6400} # multi query with top 4 docs retrieved \n",
    "\n",
    "{'context_recall': 0.8571, 'faithfulness': 0.8413, 'factual_correctness(mode=f1)': 0.7129} # multi query, parentDocRetriever, top 8 docs, new chunking method\n",
    "{'context_recall': 0.8571, 'faithfulness': 0.9083, 'factual_correctness(mode=f1)': 0.5881} # single query, parentDocRetriever, top 8 docs, new chunking method\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
